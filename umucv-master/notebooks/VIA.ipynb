{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/albertoruiz/umucv/master/images/demos/FIUM.png\" width=\"350px\" class=\"pull-right\" style=\"display: inline-block\">\n",
    "\n",
    "# Visión Artificial\n",
    "\n",
    "### 4º de Grado en Ingeniería Informática\n",
    "\n",
    "Curso 2022-2023<br>\n",
    "Prof. [*Alberto Ruiz*](http://dis.um.es/profesores/alberto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![concept map](https://raw.githubusercontent.com/albertoruiz/umucv/master/images/demos/concept_map.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Szeliski: Computer Vision: Algorithms and Applications, 2nd ed.](https://szeliski.org/Book/)\n",
    "- [OpenCV](https://opencv.org/): [tutoriales en Python](https://docs.opencv.org/4.7.0/d6/d00/tutorial_py_root.html), [documentación](https://docs.opencv.org/4.7.0/), [libro1](https://books.google.es/books?id=seAgiOfu2EIC&printsec=frontcover), [libro2](https://books.google.es/books?id=9uVOCwAAQBAJ&printsec=frontcover), [libro3](https://books.google.es/books?id=iNlOCwAAQBAJ&printsec=frontcover)\n",
    "- [Bishop: Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)\n",
    "- [Python](https://docs.python.org/3.10/), [numpy](http://www.numpy.org/), [matplotlib](http://matplotlib.org/index.html)\n",
    "- [scikit-image](http://scikit-image.org/), [scikit-learn](http://scikit-learn.org), [scipy](http://docs.scipy.org/doc/scipy/reference/)\n",
    "- [datasets](https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research#Image_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prácticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Guión de las sesiones](guionpracticas.ipynb)\n",
    "- [Preguntas frecuentes](FAQ.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El símbolo \"→\" indica enlace a los notebooks utilizados en cada capítulo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Presentación (23/1/23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[→ introducción](intro.ipynb), [→ instalación](install.ipynb), [→ Python](python.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Introducción a la asignatura\n",
    "- Repaso de Python, numpy y matplotib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introducción a la imagen digital (30/1/23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[→ imagen](imagen.ipynb), [→ gráficas](graphs.ipynb), [→ indexado/stacks](stacks.ipynb), [→ dispositivos de captura](captura.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Modelo pinhole. Campo de visión (FOV, *field of view*, parámetro $f$)\n",
    "- Imagen digital: rows, cols, depth, step. Planar or pixel order. Tipo de pixel: byte vs float\n",
    "- Color encoding: RGB vs YUV vs HSV\n",
    "- Coordendas de pixel, coordenadas normalizadas (indep de resolución), coordenadas calibradas (independiente del FOV).\n",
    "- Aspect ratio. Resize.\n",
    "- Manipulación: slice regions, \"stack\" de imágenes\n",
    "- primitivas gráficas\n",
    "- captura: webcams, cameras ip, archivos de vídeo, v4l2-ctl, etc. Load / save.\n",
    "- entornos de conda, pyqtgraph, pycharm, spyder\n",
    "- Herramientas: formatos de imagen, imagemagick, gimp, mplayer/mencoder/ffmpeg, mpv, gstreamer, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Segmentación por color (6/2/23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[→ canales de color](color.ipynb), [→ histograma](histogram.ipynb), [→ efecto chroma](chroma.ipynb), [→ segmentación por color](colorseg.ipynb), [→ cuantización de color](codebook.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Teoría del color\n",
    "- ROIs, masks, probability map, label map\n",
    "- Componentes conexas vs contornos.\n",
    "- inRange\n",
    "- Chroma key\n",
    "- Histograma, transformaciones de valor (brillo, contraste), ecualización\n",
    "- Histograma nD\n",
    "- Distancia entre histogramas. Reproyección de histograma\n",
    "- background subtraction\n",
    "- activity detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Filtros digitales (13/2/23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[→ filtros de imagen](filtros.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lineal\n",
    "\n",
    "    - convolution\n",
    "    - máscaras para paso alto, bajo, etc.\n",
    "    - separabilidad\n",
    "    - integral image, box filter\n",
    "    - dominio frecuencial\n",
    "    - filtrado inverso\n",
    "\n",
    "\n",
    "- no lineal\n",
    "\n",
    "    - mediana\n",
    "    - min, max\n",
    "    - algoritmos generales\n",
    "\n",
    "\n",
    "- Gaussian filter\n",
    "\n",
    "    - separabilidad\n",
    "    - cascading\n",
    "    - Fourier\n",
    "    - scale space\n",
    "\n",
    "\n",
    "\n",
    "- [morphological operations](http://docs.opencv.org/master/d9/d61/tutorial_py_morphological_ops.html#gsc.tab=0)\n",
    "\n",
    "    - structuring element\n",
    "    - dilate, erode\n",
    "    - open, close\n",
    "    - gradient\n",
    "    - fill holes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Análisis frecuencial (20/2/23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "[→ análisis frecuencial](fourier.ipynb), [→ filtrado inverso](inversefilt.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Reconocimiento de formas (27/2/23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[→ shapes](shapes.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- umbralización\n",
    "- análisis de regiones (componentes conexas, transformada de distancia)\n",
    "- manipulación de contornos\n",
    "- invariantes frecuenciales de forma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  6. Detección de bordes (6/3/23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[→ detección de bordes](bordes.ipynb), [→ Canny nms en C](cannyC.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gradiente: visualización como *vector field*\n",
    "- operador de Canny\n",
    "- transformada de Hough\n",
    "- Histograma de orientaciones del gradiente (HOG)\n",
    "- implementación simple de HOG\n",
    "- detección de *pedestrians*\n",
    "- face landmarks (dlib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Flujo óptico (13/3/23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[→ elipse de incertidumbre](covarianza.ipynb), [→ optical flow](harris.ipynb)\n",
    "\n",
    "- elipse de incertidumbre\n",
    "- cross-correlation\n",
    "- corners (Harris)\n",
    "- Lucas-Kanade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. *Keypoints* (20/3/23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[→ keypoints](keypoints.ipynb), [→ bag of visual words](bag-of-words.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- modelo cuadrático\n",
    "- blobs / saddle points (Hessian)\n",
    "- SIFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9a. Coordenadas homogéneas (27/3/23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos el estudio de la geometría visual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[→ perspectiva](geovis.ipynb), [→ coordenadas homogéneas](coordhomog.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformaciones lineales\n",
    "\n",
    "- espacios lineales, vectores\n",
    "- transformaciones lineales, matrices\n",
    "- producto escalar (**dot** product)\n",
    "- producto vectorial (**cross** product)\n",
    "- puntos, rectas, planos, meet & join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geometría del plano\n",
    "\n",
    "- coordenadas homogéneas\n",
    "- interpretación como rayos\n",
    "- puntos y rectas del plano\n",
    "- incidencia e intersección, dualidad\n",
    "- puntos del infinito, recta del infinito\n",
    "- manejo natural de puntos del infinito\n",
    "- horizonte de un plano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9b. Transformaciones del plano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[→ transformaciones de dominio](lookup.ipynb), [→ transformaciones del plano](transf2D.ipynb), [→ sistemas de ecuaciones](sistecs.ipynb), [→ DLT](DLT.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Desplazamientos, rotaciones, escalado uniforme, escalado general, proyectividad.\n",
    "- Grupos euclídeo, similar, afín, proyectivo.\n",
    "- Propiedades invariantes de cada grupo.\n",
    "- Representación como matriz homogénea $3\\times 3$ y tipos de matriz de cada grupo.\n",
    "- *Cross ratio* de 4 puntos en una recta. De 5 rectas.\n",
    "- Estimación de transformaciones a partir de correspondencias.\n",
    "- Aplicaciones: rectificación de planos, mosaico de imágenes.\n",
    "- Transformaciones de dominio (deformaciones), lookup table.\n",
    "\n",
    "Avanzado\n",
    "\n",
    "- Transformación de rectas. Covarianza y contravarianza.\n",
    "- Cónicas: incidencia, tangencia, (pole-polar), cónica dual, transformación.\n",
    "- Objetos invariantes en cada grupo de transformaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10a. Modelo de cámara"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[→ modelo de la cámara](camera.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Espacio proyectivo: puntos y líneas 3D, planos, grados de libertad, plano del infinito, analogía con 2D.\n",
    "- Grupos de transformaciones 3D: y sus invariantes.\n",
    "- Modelo pinhole (proyección), cámara oscura, lente.\n",
    "- Transformación de perspectiva: proyección $\\mathcal P^3 \\rightarrow\\mathcal P ^2$.\n",
    "- cámara calibrada C=PRT, 6 dof, parámetros extrínsecos o pose.\n",
    "- calibración, distorsión radial.\n",
    "- Matriz de cámara estándar $M=K[R|t]$.\n",
    "- Matriz de calibración $K$ y campo visual.\n",
    "- PnP (*pose from n points*).\n",
    "- Realidad aumentada.\n",
    "- Anatomía de la cámara\n",
    "- Rotaciones sintéticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10b. Visión estéreo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[→ stereo](stereo.ipynb), [→ stereo-challenge](stereo-challenge.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Triangulación\n",
    "- Geometría epipolar\n",
    "- Extracción de cámaras\n",
    "- Rectificación estéreo\n",
    "- Mapas de profundidad\n",
    "\n",
    "\n",
    "Experimentos\n",
    "\n",
    "- Reproduce los experimentos con un par estéreo tomado con tu propia cámara usando el *tracker* de puntos estudiado en una clase anterior.\n",
    "- Intenta poner en marcha el sistema [VisualSFM](http://ccwu.me/vsfm/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Otras técnicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[→ textura](textura.ipynb), [→ varios](varios.ipynb), [→ inpainting](ipmisc.ipynb), [→ transformada de distancia](transf-dist.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clasificación de texturas mediante *LBP* (Wang and He, 1990, [wiki](https://en.wikipedia.org/wiki/Local_binary_patterns))\n",
    "- Detección de caras mediante *adaboost* ([Viola & Jones, 2001](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.10.6807), [wiki](https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework))\n",
    "- Herramientas para OCR (*[tesseract](https://github.com/tesseract-ocr)*)\n",
    "- Herramientas para códigos de barras y QR (*[zbar](http://zbar.sourceforge.net/)*)\n",
    "- Segmentación de objetos mediante *GrabCut* ([Rother et al. 2004](https://cvg.ethz.ch/teaching/cvl/2012/grabcut-siggraph04.pdf), [tutorial](http://docs.opencv.org/3.2.0/d8/d83/tutorial_py_grabcut.html))\n",
    "- Transformada de distancia\n",
    "- Detección de elipses\n",
    "- Inpainting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. *Machine learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[→ machine learning](machine-learning.ipynb) \n",
    "\n",
    "- Repaso de *Machine Learning* y *Pattern Recognition*\n",
    "- Repaso de computación neuronal\n",
    "- Introducción a la redes convolucionales\n",
    "\n",
    "(Puedes ejecutar [este notebook en COLAB](https://colab.research.google.com/github/AlbertoRuiz/umucv/blob/master/notebooks/machine-learning.ipynb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13a. *Deep learning* en visión artificial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[→ modelos avanzados](deep.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Modelos preentrenados\n",
    "- YOLO\n",
    "- face recognition\n",
    "- openpose (body landmarks)\n",
    "- Transfer learning\n",
    "- Data augmentation\n",
    "- UNET\n",
    "- Visual transformer\n",
    "- MLP mixer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13b. Síntesis de imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[→ bottleneck](bottleneck.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Autoencoder\n",
    "- VAE\n",
    "- GAN\n",
    "- Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos de código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobación inicial\n",
    "\n",
    "1. [`hello.py`](../code/hello.py): lee imagen de archivo, la reescala, muestra y sobreescribe un texto.\n",
    "\n",
    "1. [`webcam.py`](../code/webcam.py): muestra la secuencia de imágenes capturadas por una webcam.\n",
    "\n",
    "1. [`2cams.py`](../code/2cams.py): combina las imágenes tomadas por dos cámaras.\n",
    "\n",
    "1. [`stream.py`](../code/stream.py): ejemplo de uso de la fuente genérica de imágenes.\n",
    "\n",
    "1. [`surface.py`](../code/surface.py): superficie 3D de niveles de gris en vivo usando pyqtgraph.\n",
    "\n",
    "1. [`facemesh.py`](../code/facemesh.py): malla de una cara usando mediapipe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilidades\n",
    "\n",
    "1. [`save_video.py`](../code/save_video.py): ejemplo de uso de la utilidad de grabación de vídeo.\n",
    "\n",
    "1. [`mouse.py`](../code/mouse.py), [`medidor.py`](../code/medidor.py): ejemplo de captura de eventos de ratón.\n",
    "\n",
    "1. [`roi.py`](../code/roi.py): ejemplo de selección de región rectangular.\n",
    "\n",
    "1. [`trackbar.py`](../code/trackbar.py): ejemplo de parámetro interactivo.\n",
    "\n",
    "1. [`help_window.py`](../code/help_window.py): ejemplo de ventana de ayuda.\n",
    "\n",
    "1. [`wzoom.py`](../code/wzoom.py): ejemplo de ventana con zoom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actividad\n",
    "\n",
    "1. [`deque.py`](../code/deque.py): procesamiento de las $n$ imágenes más recientes.\n",
    "\n",
    "1. [`backsub0.py`](../code/backsub0.py), [`backsub.py`](../code/backsub.py): eliminación de fondo mediante MOG2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Color\n",
    "\n",
    "1. [`histogram.py`](../code/histogram.py): histograma en vivo con opencv.\n",
    "\n",
    "1. [`histogram2.py`](../code/histogram2.py): histograma en vivo con matplotlib.\n",
    "\n",
    "1. [`inrange0.py`](../code/inrange0.py), [`inrange.py`](../code/inrange.py): umbralización de color, máscaras, componentes conexas y contornos.\n",
    "\n",
    "1. [`reprohist.py`](../code/reprohist.py),  [`mean-shift.py`](../code/mean-shift.py), [`camshift.py`](../code/camshift.py): reproyección de histograma y tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [`surface2.py`](../code/surface2.py): superficie 3D de niveles de gris  suavizada y manejo de teclado con pyqtgraph y opengl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [`server.py`](../code/server.py): ejemplo de servidor web de imágenes capturadas con la webcam.\n",
    "\n",
    "1. [`mjpegserver.py`](../code/mjpegserver.py): servidor de secuencias de video en formato mjpeg.\n",
    "\n",
    "1. [`bot`](../code/bot): bots de [Telegram](https://python-telegram-bot.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [`grabcut.py`](../code/grabcut.py): segmentación de objetos interactiva mediante GrabCut.\n",
    "\n",
    "1. [`spectral.py`](../code/spectral.py): FFT en vivo.\n",
    "\n",
    "1. [`thread`](../code/thread): captura y procesamiento concurrente.\n",
    "\n",
    "1. [`testC.py`](../code/testC.py), [`inC`](../code/inC): Interfaz C-numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [`hog/pedestrian.py`](../code/hog/pedestrian.py): detector de peatones de opencv.\n",
    "\n",
    "1. [`hog/facelandmarks.py`](../code/hog/facelandmarks.py): detector de caras y landmarks de dlib.\n",
    "\n",
    "1. [`hog/hog0.py`](../code/hog/hog0.py): experimentos con hog.\n",
    "\n",
    "1. [`regressor.py`](../code/regressor.py): predictor directo de la posición de una región.\n",
    "\n",
    "1. [`crosscorr.py`](../code/crosscorr.py): ejemplo de match template.\n",
    "\n",
    "1. [`kcf.py`](../code/crosscorr.py): ejemplo de discriminative correlation filter.\n",
    "\n",
    "1. [`LK/*.py`](../code/LK): seguimiento de puntos con el método de Lucas-Kanade.\n",
    "\n",
    "1. [`SIFT/*.py`](../code/sift.py): demostración de la detección de keypoints y búsqueda de coincidencias en imágenes en vivo.\n",
    "\n",
    "1. [`shape/*.py`](../code/shape): reconocimiento de formas mediante descriptores frecuenciales.\n",
    "\n",
    "1. [`ocr.py`](../code/ocr.py): reconocimiento de caracteres impresos con tesseract/tesserocr sobre imagen en vivo.\n",
    "\n",
    "1. [`zbardemo.py`](../code/zbardemo.py): detección de códigos de barras y QR sobre imagen en vivo.\n",
    "\n",
    "1. [`code/DL`](../code/DL): Modelos avanzados de deep learning para visión artificial (inception, YOLO, FaceDeep, openpose).\n",
    "\n",
    "1. [`code/polygons`](../code/polygons) y [`code/elipses`](../code/elipses): Rectificación de planos en base a marcadores artificiales.\n",
    "\n",
    "1. [`stitcher.py`](../code/stitcher.py): construcción automática de panoramas.\n",
    "\n",
    "1. [`code/pose`](../code/pose): estimación de la matriz de cámara y realidad aumentada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La entrega de los ejercicios se hará en una tarea del aula virtual dentro de un archivo comprimido.\n",
    "\n",
    "Debe incluir el **código** completo .py de todos los ejercicios, los ficheros auxiliares (siempre que no sean muy pesados), y una **memoria** con una **explicación** detallada de las soluciones propuestas, las funciones o trozos de código más importantes, y **resultados** de funcionamiento con imágenes de evaluación **originales** en forma de pantallazos o videos de demostración. También es conveniente incluir información sobre tiempos de cómputo, limitaciones de las soluciones propuestas y casos de fallo.\n",
    "\n",
    "La memoria se presentará en un formato **pdf** o **jupyter** (en este caso se debe adjuntar también una versión **html** del notebook completamente evaluado).\n",
    "\n",
    "Lo importante, además de la evaluación de la asignatura, es que os quede un buen documento de referencia para el futuro.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los ejercicios propuestos se irán incluyendo aquí a lo largo del curso. (Como orientación, aquí están los [ejercicios del curso anterior](ejercicios-curso-anterior.ipynb).)!puthon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obligatorios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CALIBRACIÓN**. a) Realiza una calibración precisa de tu cámara mediante múltiples imágenes de un *chessboard*. b) Haz una calibración aproximada con un objeto de tamaño conocido y compara con el resultado anterior. c) Determina a qué altura hay que poner la cámara para obtener una vista cenital completa de un campo de baloncesto. d) Haz una aplicación para medir el ángulo que definen dos puntos marcados con el ratón en la imagen. e) Opcional: determina la posición aproximada desde la que se ha tomado una foto a partir ángulos observados respecto a puntos de referencia conocidos. [Más informacion](imagen.ipynb#Calibración)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ACTIVIDAD**. Construye un detector de movimiento en una región de interés de la imagen marcada manualmente. Guarda 2 ó 3 segundos de la secuencia detectada en un archivo de vídeo. Muestra el objeto seleccionado anulando el fondo. Impleméntalo de dos maneras: a) Utilizando un substractor de fondo de opencv como en los ejemplos backsub0.py y backsub.py. b) Mediante un procedimiento sencillo que construya un modelo de fondo con frames anteriores y compare con el actual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COLOR**. Construye un contador de objetos que tengan un color característico en la escena, simplemente pinchando con el ratón en dos o tres de ellos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FILTROS**. Amplía el código de la práctica 4 para mostrar en vivo el efecto de diferentes filtros, seleccionando con el teclado el filtro deseado y modificando sus parámetros (p.ej. el nivel de suavizado) con trackbars. Aplica el filtro en un ROI para comparar el resultado con el resto de la imagen ([ejemplo](../images/demos/ej-c4.png)). Opcional: **a)** Comprueba la propiedad de \"cascading\" del filtro gaussiano. **b)** Comprueba la propiedad de \"separabilidad\" del filtro gaussiano. **c)** Implementa en Python dede cero (usando bucles) el algoritmo de convolución con una máscara general y compara su eficiencia con la versión de OpenCV. **c)** Impleméntalo en C y haz un \"wrapper\" para utilizarlo desde Python (consulta al profesor). **d)** Implementa el box filter con la imagen integral. **e)** Añade la posibilidad de seleccionar varios filtros para aplicarlos sucesivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SIFT**. Escribe una aplicación de reconocimiento de objetos (p. ej. carátulas de CD, portadas de libros, cuadros de pintores, etc.) con la webcam basada en el número de coincidencias de *keypoints*. [Más información](FAQ.ipynb#Ejercicio-SIFT)."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**RECTIF**. Rectifica la imagen de un plano para medir distancias (tomando manualmente referencias conocidas). Por ejemplo, mide la distancia entre las monedas en coins.png o la distancia a la que se realiza el disparo en gol-eder.png. Las coordenadas reales de los puntos de referencia y sus posiciones en la imagen deben pasarse como parámetro en un archivo de texto. Aunque puedes mostrar la imagen rectificada para comprobar las operaciones, debes marcar los puntos y mostrar el resultado sobre la imagen original. Verifica los resultados con imágenes originales tomadas por ti."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "RA. Crea un efecto de realidad aumentada en el que el usuario interactúe con los objetos virtuales. Por ejemplo, haciendo que un objeto se desplace hacia un punto señalado con el ratón."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "en",
    "es"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "es",
   "targetLang": "en",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
